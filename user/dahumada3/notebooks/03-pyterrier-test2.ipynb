{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb740334-d9c1-4a60-b6dc-925e15cd017a",
   "metadata": {},
   "source": [
    "### Pyterrier test for hackaton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "810f328c-184c-4940-8bf1-5c0cf95283a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-3.3.2-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\david\\.conda\\envs\\eriskenv\\lib\\site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\david\\.conda\\envs\\eriskenv\\lib\\site-packages (from datasets) (1.26.3)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Downloading pyarrow-19.0.1-cp310-cp310-win_amd64.whl.metadata (3.4 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\david\\.conda\\envs\\eriskenv\\lib\\site-packages (from datasets) (2.1.4)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\david\\.conda\\envs\\eriskenv\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Collecting tqdm>=4.66.3 (from datasets)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp310-cp310-win_amd64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in c:\\users\\david\\.conda\\envs\\eriskenv\\lib\\site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.3.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\david\\.conda\\envs\\eriskenv\\lib\\site-packages (from datasets) (3.9.3)\n",
      "Collecting huggingface-hub>=0.24.0 (from datasets)\n",
      "  Downloading huggingface_hub-0.29.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\david\\.conda\\envs\\eriskenv\\lib\\site-packages (from datasets) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\david\\.conda\\envs\\eriskenv\\lib\\site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\david\\.conda\\envs\\eriskenv\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\david\\.conda\\envs\\eriskenv\\lib\\site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\david\\.conda\\envs\\eriskenv\\lib\\site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\david\\.conda\\envs\\eriskenv\\lib\\site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\david\\.conda\\envs\\eriskenv\\lib\\site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\users\\david\\.conda\\envs\\eriskenv\\lib\\site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\david\\.conda\\envs\\eriskenv\\lib\\site-packages (from huggingface-hub>=0.24.0->datasets) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\david\\.conda\\envs\\eriskenv\\lib\\site-packages (from requests>=2.32.2->datasets) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\david\\.conda\\envs\\eriskenv\\lib\\site-packages (from requests>=2.32.2->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\david\\.conda\\envs\\eriskenv\\lib\\site-packages (from requests>=2.32.2->datasets) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\david\\.conda\\envs\\eriskenv\\lib\\site-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: colorama in c:\\users\\david\\.conda\\envs\\eriskenv\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\david\\.conda\\envs\\eriskenv\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\david\\.conda\\envs\\eriskenv\\lib\\site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\david\\.conda\\envs\\eriskenv\\lib\\site-packages (from pandas->datasets) (2023.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\david\\.conda\\envs\\eriskenv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Downloading datasets-3.3.2-py3-none-any.whl (485 kB)\n",
      "   ---------------------------------------- 0.0/485.4 kB ? eta -:--:--\n",
      "   ---------- ----------------------------- 122.9/485.4 kB 3.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 471.0/485.4 kB 5.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 485.4/485.4 kB 5.1 MB/s eta 0:00:00\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "   ---------------------------------------- 0.0/116.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 116.3/116.3 kB 6.6 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.29.1-py3-none-any.whl (468 kB)\n",
      "   ---------------------------------------- 0.0/468.0 kB ? eta -:--:--\n",
      "   --------------------------------------- 468.0/468.0 kB 14.8 MB/s eta 0:00:00\n",
      "Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "   ---------------------------------------- 0.0/134.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 134.8/134.8 kB 8.3 MB/s eta 0:00:00\n",
      "Downloading pyarrow-19.0.1-cp310-cp310-win_amd64.whl (25.3 MB)\n",
      "   ---------------------------------------- 0.0/25.3 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 1.3/25.3 MB 41.3 MB/s eta 0:00:01\n",
      "   ----- ---------------------------------- 3.5/25.3 MB 43.9 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 5.6/25.3 MB 44.8 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 8.4/25.3 MB 48.8 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 11.5/25.3 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 13.8/25.3 MB 59.5 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 14.1/25.3 MB 54.4 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 14.1/25.3 MB 54.4 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 17.5/25.3 MB 40.9 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 19.6/25.3 MB 38.5 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 21.1/25.3 MB 34.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 23.5/25.3 MB 40.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  25.3/25.3 MB 50.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  25.3/25.3 MB 50.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  25.3/25.3 MB 50.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  25.3/25.3 MB 50.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  25.3/25.3 MB 50.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  25.3/25.3 MB 50.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  25.3/25.3 MB 50.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  25.3/25.3 MB 50.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 25.3/25.3 MB 17.7 MB/s eta 0:00:00\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading xxhash-3.5.0-cp310-cp310-win_amd64.whl (30 kB)\n",
      "Installing collected packages: xxhash, tqdm, pyarrow, dill, multiprocess, huggingface-hub, datasets\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.66.1\n",
      "    Uninstalling tqdm-4.66.1:\n",
      "      Successfully uninstalled tqdm-4.66.1\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 12.0.1\n",
      "    Uninstalling pyarrow-12.0.1:\n",
      "      Successfully uninstalled pyarrow-12.0.1\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.9\n",
      "    Uninstalling dill-0.3.9:\n",
      "      Successfully uninstalled dill-0.3.9\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.21.4\n",
      "    Uninstalling huggingface-hub-0.21.4:\n",
      "      Successfully uninstalled huggingface-hub-0.21.4\n",
      "Successfully installed datasets-3.3.2 dill-0.3.8 huggingface-hub-0.29.1 multiprocess-0.70.16 pyarrow-19.0.1 tqdm-4.67.1 xxhash-3.5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\david\\.conda\\envs\\eriskenv\\Lib\\site-packages\\~-arrow'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "mlflow 2.5.0 requires pyarrow<13,>=4.0.0, but you have pyarrow 19.0.1 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161dc044-9542-4454-bcf0-9e50679a86f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88aa9ce1-cfb2-40c5-90a3-0bf47938114a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_30492\\4123888700.py:5: DeprecationWarning: Call to deprecated function (or staticmethod) started. (use pt.java.started() instead) -- Deprecated since version 0.11.0.\n",
      "  if not pt.started():\n",
      "Java started and loaded: pyterrier.java, pyterrier.terrier.java [version=5.11 (build: craig.macdonald 2025-01-13 21:29), helper_version=0.0.8]\n",
      "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_30492\\4123888700.py:6: DeprecationWarning: Call to deprecated method pt.init(). Deprecated since version 0.11.0.\n",
      "java is now started automatically with default settings. To force initialisation early, run:\n",
      "pt.java.init() # optional, forces java initialisation\n",
      "  pt.init()\n"
     ]
    }
   ],
   "source": [
    "import pyterrier as pt\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "if not pt.started():\n",
    "    pt.init()\n",
    "\n",
    "# Load the dataset from Hugging Face\n",
    "dataset = load_dataset(\"AGBonnet/augmented-clinical-notes\", split=\"train\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28d236f8-f5ee-4fb3-9465-82aadfb963d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['idx', 'note', 'full_note', 'conversation', 'summary'],\n",
       "    num_rows: 30000\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e935111a-1d35-425e-bfb7-6c471768818d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docno</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>30000</td>\n",
       "      <td>30000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>30000</td>\n",
       "      <td>24341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>155216</td>\n",
       "      <td>A 54-year-old Caucasian female proceeded to ou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         docno                                               text\n",
       "count    30000                                              30000\n",
       "unique   30000                                              24341\n",
       "top     155216  A 54-year-old Caucasian female proceeded to ou...\n",
       "freq         1                                                 10"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Choose the 'full_note' field for indexing.\n",
    "# Alternatively, you could use 'note' or combine fields if that better fits your use case.\n",
    "docs = [{'docno': row['idx'], 'text': row['full_note']} for row in dataset]\n",
    "\n",
    "# Create a DataFrame from the records.\n",
    "df = pd.DataFrame(docs)\n",
    "df.describe()\n",
    "\n",
    "# # Index the documents using PyTerrier's IterDictIndexer.\n",
    "# indexer = pt.index.IterDictIndexer(\"./pt_index\")\n",
    "# index_ref = indexer.index(df.to_dict(orient='records'), fields=['docno', 'text'])\n",
    "\n",
    "# indexer = pt.index.IterDictIndexer(\"./pt_index\", fields=[\"docno\", \"text\"], text_attrs=\"text\")\n",
    "# index_ref = indexer.index(df.to_dict(orient='records'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e21c02f2-35cd-48c8-b7c0-3a8a62038931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Deleted existing index at: D:\\SRC\\DS@GT\\eRisk25\\eRisk25-datasets\\pt_index\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Define an absolute index path (adjust this to a valid writable location)\n",
    "index_path = r\"D:\\SRC\\DS@GT\\eRisk25\\eRisk25-datasets\\pt_index\"\n",
    "\n",
    "# ✅ Delete existing index if needed\n",
    "if os.path.exists(index_path):\n",
    "    shutil.rmtree(index_path)\n",
    "    print(f\"✅ Deleted existing index at: {index_path}\")    \n",
    "\n",
    "\n",
    "indexer = pt.index.IterDictIndexer(index_path)\n",
    "index_ref = indexer.index(df.to_dict(orient='records'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "704990d6-3f60-4340-81bc-96191689593e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_30492\\2155857068.py:2: DeprecationWarning: Call to deprecated class BatchRetrieve. (use pt.terrier.Retriever() instead) -- Deprecated since version 0.11.0.\n",
      "  bm25 = pt.BatchRetrieve(index_ref, wmodel=\"BM25\")\n"
     ]
    }
   ],
   "source": [
    "# Initialize BM25 retrieval model.\n",
    "bm25 = pt.BatchRetrieve(index_ref, wmodel=\"BM25\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70ee54eb-45a9-4dbc-9fab-557f8ed546d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\david\\.conda\\envs\\eriskenv\\lib\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c31d8ad5d3ab4f468cef9f5ea40751b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2104a19e308b4d58adee91695a0f0aed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ada5df863cb3407499426fbd8548f4ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de2eee8e6ee242c48039cc939883e7de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.84k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "131fd68a0a5d46f69d5a9250c0bd0e9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/892M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyterrier_t5 import MonoT5ReRanker\n",
    "monoT5 = MonoT5ReRanker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce87e0b9-8028-4d34-b0be-6e7e3c601c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = bm25 >> monoT5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "064e6531-7e33-4c2b-8738-60044558cf39",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\.conda\\envs\\eriskenv\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3791\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3790\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3791\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3792\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:152\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:181\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'text'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpatients with hypertension\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(results\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m10\u001b[39m))\n",
      "File \u001b[1;32m~\\.conda\\envs\\eriskenv\\lib\\site-packages\\pyterrier\\transformer.py:229\u001b[0m, in \u001b[0;36mTransformer.search\u001b[1;34m(self, query, qid, sort)\u001b[0m\n\u001b[0;32m    227\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m    228\u001b[0m queryDf \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame([[qid, query]], columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqid\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m--> 229\u001b[0m rtr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqueryDf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqid\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m rtr\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrank\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m rtr\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[0;32m    231\u001b[0m     rtr \u001b[38;5;241m=\u001b[39m rtr\u001b[38;5;241m.\u001b[39msort_values([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqid\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrank\u001b[39m\u001b[38;5;124m\"\u001b[39m], ascending\u001b[38;5;241m=\u001b[39m[\u001b[38;5;28;01mTrue\u001b[39;00m,\u001b[38;5;28;01mTrue\u001b[39;00m])\n",
      "File \u001b[1;32m~\\.conda\\envs\\eriskenv\\lib\\site-packages\\pyterrier\\_ops.py:372\u001b[0m, in \u001b[0;36mCompose.transform\u001b[1;34m(self, inp)\u001b[0m\n\u001b[0;32m    370\u001b[0m out \u001b[38;5;241m=\u001b[39m inp\n\u001b[0;32m    371\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transformers:\n\u001b[1;32m--> 372\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32m~\\.conda\\envs\\eriskenv\\lib\\site-packages\\pyterrier_t5\\__init__.py:38\u001b[0m, in \u001b[0;36mMonoT5ReRanker.transform\u001b[1;34m(self, run)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform\u001b[39m(\u001b[38;5;28mself\u001b[39m, run):\n\u001b[0;32m     37\u001b[0m     scores \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 38\u001b[0m     queries, texts \u001b[38;5;241m=\u001b[39m run[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[43mrun\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext_field\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     39\u001b[0m     it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(queries), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size)\n\u001b[0;32m     40\u001b[0m     prompts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mbatch_encode_plus([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRelevant:\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size)], return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlongest\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\.conda\\envs\\eriskenv\\lib\\site-packages\\pandas\\core\\frame.py:3893\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3891\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3892\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3893\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3895\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\.conda\\envs\\eriskenv\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3798\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3793\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3794\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3795\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3796\u001b[0m     ):\n\u001b[0;32m   3797\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3798\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3799\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3800\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3801\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3802\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3803\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'text'"
     ]
    }
   ],
   "source": [
    "query = \"patients with hypertension\"\n",
    "results = pipeline.search(query)\n",
    "print(results.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8567cc59-5ff1-4e65-9c37-a8a1139aa543",
   "metadata": {},
   "source": [
    "This error occurs because the reranker expects the input DataFrame to include a column named \"text\", but your retrieval results don't have it. When BM25 retrieves documents from your index, it doesn't automatically include the full document text unless you explicitly request it via the metadata parameter.\n",
    "\n",
    "To fix this, update your BM25 initialization to include the \"text\" field. For example:\n",
    "\n",
    "python\n",
    "Copy\n",
    "bm25 = pt.BatchRetrieve(index_ref, wmodel=\"BM25\", metadata=[\"text\"])\n",
    "This tells BM25 to attach the \"text\" field (i.e., the full clinical note) from your index to each retrieved document. With this change, the subsequent reranker (which expects a \"text\" column) should work correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9aed5b0-c9c4-4b06-8e0c-16cf79282dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_30492\\1657241519.py:1: DeprecationWarning: Call to deprecated class BatchRetrieve. (use pt.terrier.Retriever() instead) -- Deprecated since version 0.11.0.\n",
      "  bm25 = pt.BatchRetrieve(index_ref, wmodel=\"BM25\", metadata=[\"text\"])\n"
     ]
    }
   ],
   "source": [
    "bm25 = pt.BatchRetrieve(index_ref, wmodel=\"BM25\", metadata=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "18d667cd-fe1b-480d-a30b-1cb3a43b3932",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "monoT5: 100%|███████████████████████████████████████████████████████████████████| 250/250 [00:25<00:00,  9.81batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   qid  docid text                       query      score  rank\n",
      "1    1    350       patients with hypertension -11.283523     0\n",
      "3    1   8847       patients with hypertension -11.283523     1\n",
      "5    1  14075       patients with hypertension -11.283523     2\n",
      "7    1  19041       patients with hypertension -11.283523     3\n",
      "9    1  19248       patients with hypertension -11.283523     4\n",
      "11   1  20468       patients with hypertension -11.283523     5\n",
      "13   1  27145       patients with hypertension -11.283523     6\n",
      "15   1  17649       patients with hypertension -11.283523     7\n",
      "17   1  11276       patients with hypertension -11.283523     8\n",
      "19   1  23561       patients with hypertension -11.283523     9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Build the two-stage pipeline.\n",
    "pipeline = bm25 >> monoT5\n",
    "\n",
    "# Run a query.\n",
    "query = \"patients with hypertension\"\n",
    "results = pipeline.search(query)\n",
    "print(results.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a12e37d-a1d8-4282-b45e-f1484bd287a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
