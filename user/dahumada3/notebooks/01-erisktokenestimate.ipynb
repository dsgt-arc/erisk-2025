{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "490070df-2a2d-4fbb-9ad6-47ca3397c27e",
   "metadata": {},
   "source": [
    "### Estimate the amount of tokens for the erisk 2025 datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2cf3805-4559-494b-a816-130251f06f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive D is Data\n",
      " Volume Serial Number is 6A96-9CEA\n",
      "\n",
      " Directory of D:\\SRC\\DS@GT\\eRisk25\\eRisk25-datasets\n",
      "\n",
      "02/14/2025  01:36 PM    <DIR>          .\n",
      "02/14/2025  01:36 PM    <DIR>          ..\n",
      "02/14/2025  12:54 PM    <DIR>          .ipynb_checkpoints\n",
      "02/09/2025  03:48 PM           826,340 00-erisk25task1EDA.ipynb\n",
      "02/14/2025  01:34 PM             2,564 01-erisktokenestimate.ipynb\n",
      "02/06/2025  12:09 AM     1,977,791,083 merged_output.parquet\n",
      "02/06/2025  09:54 PM       231,616,082 merged_output_2023.parquet\n",
      "02/06/2025  10:29 PM     2,816,315,923 merged_output_2024.parquet\n",
      "02/07/2025  01:41 PM            55,251 most_common_bigrams_chart.png\n",
      "02/07/2025  01:36 PM            37,899 most_common_words_chart.png\n",
      "02/04/2025  10:02 PM             1,572 t1_parquet.py\n",
      "02/03/2025  11:00 PM    <DIR>          task1-symptom-ranking\n",
      "02/06/2025  10:08 PM             1,716 task1_parquetmerge.py\n",
      "02/06/2025  10:04 PM             2,104 task1_trec2parquet.py\n",
      "01/16/2025  08:18 PM    <DIR>          task2-contextualized-early-depression\n",
      "02/06/2025  10:37 PM    <DIR>          test-parquet\n",
      "02/04/2025  10:54 PM               103 testimport.py\n",
      "02/14/2025  01:36 PM             3,885 Untitled.ipynb\n",
      "              12 File(s)  5,026,654,522 bytes\n",
      "               6 Dir(s)  115,389,378,560 bytes free\n"
     ]
    }
   ],
   "source": [
    "!dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3311886-e820-4368-813b-dc877baef93e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Processing merged_output.parquet ...\n",
      "‚úÖ Completed: merged_output.parquet\n",
      "üîÑ Processing merged_output_2023.parquet ...\n",
      "‚úÖ Completed: merged_output_2023.parquet\n",
      "üîÑ Processing merged_output_2024.parquet ...\n",
      "‚úÖ Completed: merged_output_2024.parquet\n",
      "\n",
      "‚úÖ Total documents processed: 37360334\n",
      "üìä Estimated total token count (GPT-3.5 tokenizer): 713,612,490\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import tiktoken\n",
    "import os\n",
    "\n",
    "# List of Parquet files\n",
    "parquet_files = [\n",
    "    \"merged_output.parquet\",\n",
    "    \"merged_output_2023.parquet\",\n",
    "    \"merged_output_2024.parquet\"\n",
    "]\n",
    "\n",
    "# Load GPT-3.5 tokenizer\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "# Initialize counters\n",
    "total_token_count = 0\n",
    "total_documents = 0\n",
    "\n",
    "# Process each Parquet file\n",
    "for file in parquet_files:\n",
    "    if not os.path.exists(file):\n",
    "        print(f\"‚ö†Ô∏è File not found: {file}\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"üîÑ Processing {file} ...\")\n",
    "    \n",
    "    # Open Parquet file\n",
    "    parquet_table = pq.ParquetFile(file)\n",
    "\n",
    "    # Process file in row groups\n",
    "    for row_group_index in range(parquet_table.num_row_groups):\n",
    "        # Read a single row group (to avoid memory overload)\n",
    "        batch = parquet_table.read_row_group(row_group_index, columns=[\"TEXT\"]).to_pandas()\n",
    "\n",
    "        # Tokenize and count tokens\n",
    "        total_token_count += sum(len(tokenizer.encode(text)) for text in batch[\"TEXT\"].dropna())\n",
    "        total_documents += len(batch)\n",
    "\n",
    "    print(f\"‚úÖ Completed: {file}\")\n",
    "\n",
    "# Final results\n",
    "print(f\"\\n‚úÖ Total documents processed: {total_documents}\")\n",
    "print(f\"üìä Estimated total token count (GPT-3.5 tokenizer): {total_token_count:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e143fe-9ea0-4da8-9e0a-2726f02ca707",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
