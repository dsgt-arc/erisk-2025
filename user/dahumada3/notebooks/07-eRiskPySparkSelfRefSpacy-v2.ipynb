{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5152580",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00-erisk25task1EDA.ipynb\t       05-eRiskPySparkSelfRefFiltering.ipynb\n",
      "01-erisktokenestimate.ipynb\t       06-eRiskPySparkSelfRefSpacy.ipynb\n",
      "03-pyterrier-test2.ipynb\t       07-eRiskPySparkSelfRefSpacy-v2.ipynb\n",
      "04-eRiskAnalysisSelfReferential.ipynb\n",
      "/storage/home/hcoda1/6/dahumada3/clef/erisk-2025/user/dahumada3/notebooks\n"
     ]
    }
   ],
   "source": [
    "!ls\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "916cfeae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/28 02:36:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/03/28 02:36:57 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://atl1-1-03-003-3-2.pace.gatech.edu:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[24]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>clef</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1555259b57b0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"/storage/home/hcoda1/6/dahumada3/clef/erisk-2025\")\n",
    "\n",
    "from erisk.spark import get_spark\n",
    "\n",
    "spark = get_spark()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b2a5214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set paths\n",
    "LABELS_MAJ_PATH = \"/storage/home/hcoda1/6/dahumada3/erisk_shared/raw/training_data/2023/g_qrels_majority_2.csv\"\n",
    "LABELS_CONS_PATH = \"/storage/home/hcoda1/6/dahumada3/erisk_shared/raw/training_data/2023/g_rels_consenso.csv\"\n",
    "PARQUET_DIR = \"/storage/home/hcoda1/6/dahumada3/erisk_shared/parquet/training_data/2023/partitions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c512aba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r\n",
      "\r\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DOCNO: string (nullable = true)\n",
      " |-- TEXT: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------------------------------------------------------------------------------+\n",
      "|DOCNO       |TEXT                                                                                 |\n",
      "+------------+-------------------------------------------------------------------------------------+\n",
      "|s_2457_109_0|LADWP, Inyo agree to test run on well 395                                            |\n",
      "|s_2457_110_0|State representatives recognize NIHDs District of Year designation                   |\n",
      "|s_2457_111_0|Small plane crashes en route from Bishop to Nanaimo, BC (Canada)                     |\n",
      "|s_2457_112_0|Audio and video production professionals, as an example: https://youtu.be/jv5HIrOrn2o|\n",
      "|s_2457_113_0|Sure, its a professional powerhouse for audio and video production professionals.    |\n",
      "+------------+-------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(PARQUET_DIR)\n",
    "df.printSchema()\n",
    "df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e06f882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------+\n",
      "|TEXT                                                                             |\n",
      "+---------------------------------------------------------------------------------+\n",
      "|LADWP, Inyo agree to test run on well 395                                        |\n",
      "|State representatives recognize NIHDs District of Year designation               |\n",
      "|Small plane crashes en route from Bishop to Nanaimo, BC (Canada)                 |\n",
      "|Audio and video production professionals, as an example:                         |\n",
      "|Sure, its a professional powerhouse for audio and video production professionals.|\n",
      "+---------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:=================================================>       (21 + 3) / 24]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining rows: 4264560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, trim, regexp_replace\n",
    "\n",
    "# Remove empty/whitespace-only posts\n",
    "df_clean = df.filter(trim(col(\"TEXT\")) != \"\")\n",
    "\n",
    "# Remove URLs\n",
    "df_clean = df_clean.withColumn(\"TEXT\", regexp_replace(\"TEXT\", r\"http\\S+|www\\S+\", \"\"))\n",
    "\n",
    "# Remove Markdown-style links like [text](url)\n",
    "df_clean = df_clean.withColumn(\"TEXT\", regexp_replace(\"TEXT\", r\"\\[.*?\\]\\(.*?\\)\", \"\"))\n",
    "\n",
    "# Remove common artifacts like 'gt;' (HTML > symbol)\n",
    "df_clean = df_clean.withColumn(\"TEXT\", regexp_replace(\"TEXT\", \"gt;\", \"\"))\n",
    "\n",
    "df_clean.select(\"TEXT\").show(5, truncate=False)\n",
    "print(\"Remaining rows:\", df_clean.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0cbd650b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lower\n",
    "\n",
    "SELF_REF_PATTERN = (\n",
    "    r\"\\b(\"\n",
    "    r\"i|me|my|mine|myself|\"  # basic forms\n",
    "    r\"i'm|im|\"  # I'm and Im\n",
    "    r\"i’ve|ive|\"  # I've and Ive\n",
    "    r\"i’d|id|\"  # I'd and Id\n",
    "    r\"i’ll|ill|\"  # I'll and Ill\n",
    "    r\"i am|i was\"  # long forms\n",
    "    r\")\\b\"\n",
    ")\n",
    "\n",
    "df_with_flag = df_clean.withColumn(\n",
    "    \"is_self_ref\", lower(col(\"TEXT\")).rlike(SELF_REF_PATTERN)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fdb6867a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:===================================================>    (22 + 2) / 24]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self-referential posts: 1,218,201 out of 4,264,560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "[Stage 11:=====================================================>  (23 + 1) / 24]\r\n",
      "\r\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "self_ref_df = df_with_flag.filter(col(\"is_self_ref\"))\n",
    "\n",
    "total_count = df_with_flag.count()\n",
    "self_ref_count = self_ref_df.count()\n",
    "\n",
    "print(f\"Self-referential posts: {self_ref_count:,} out of {total_count:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2dc790a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+-----------+\n",
      "|       DOCNO|                TEXT|is_self_ref|\n",
      "+------------+--------------------+-----------+\n",
      "|s_2457_109_0|LADWP, Inyo agree...|      false|\n",
      "|s_2457_110_0|State representat...|      false|\n",
      "|s_2457_111_0|Small plane crash...|      false|\n",
      "|s_2457_112_0|Audio and video p...|      false|\n",
      "|s_2457_113_0|Sure, its a profe...|      false|\n",
      "+------------+--------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_with_flag.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "229131d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "\n",
    "def process_partition_with_spacy(partition):\n",
    "    import spacy\n",
    "\n",
    "    # Load spaCy once per partition\n",
    "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"parser\"])\n",
    "\n",
    "    # Self-referential word set (lowercase)\n",
    "    SELF_REF_WORDS = {\n",
    "        \"i\",\n",
    "        \"me\",\n",
    "        \"my\",\n",
    "        \"mine\",\n",
    "        \"myself\",\n",
    "        \"i'm\",\n",
    "        \"im\",\n",
    "        \"i’ve\",\n",
    "        \"ive\",\n",
    "        \"i'd\",\n",
    "        \"id\",\n",
    "        \"i’ll\",\n",
    "        \"ill\",\n",
    "        \"i am\",\n",
    "        \"i was\",\n",
    "    }\n",
    "\n",
    "    for row in partition:\n",
    "        text = row[\"TEXT\"]\n",
    "\n",
    "        doc = nlp(text.lower())\n",
    "        tokens = [token.text for token in doc if not token.is_space]\n",
    "\n",
    "        if not tokens:\n",
    "            ratio = 0.0\n",
    "        else:\n",
    "            self_ref_count = sum(1 for token in tokens if token in SELF_REF_WORDS)\n",
    "            ratio = self_ref_count / len(tokens)\n",
    "\n",
    "        yield Row(**row.asDict(), self_ref_ratio=ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc5ff78c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 21:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ Completed in 7.09 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Your Spark transformation\n",
    "df_with_ratio = df_with_flag.rdd.mapPartitions(process_partition_with_spacy).toDF()\n",
    "\n",
    "end = time.time()\n",
    "print(f\"⏱️ Completed in {end - start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "45159f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 23:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+-----------+-------------------+\n",
      "|       DOCNO|                TEXT|is_self_ref|     self_ref_ratio|\n",
      "+------------+--------------------+-----------+-------------------+\n",
      "|s_2457_109_0|LADWP, Inyo agree...|      false|                0.0|\n",
      "|s_2457_110_0|State representat...|      false|                0.0|\n",
      "|s_2457_111_0|Small plane crash...|      false|                0.0|\n",
      "|s_2457_112_0|Audio and video p...|      false|                0.0|\n",
      "|s_2457_113_0|Sure, its a profe...|      false|                0.0|\n",
      "|s_2457_113_1|I have no use for...|       true|0.07142857142857142|\n",
      "|s_2457_113_2|But I wont preten...|       true|0.06666666666666667|\n",
      "|s_2457_113_3|I could use a che...|       true|0.17647058823529413|\n",
      "|s_2457_113_4|But industry prof...|      false|                0.0|\n",
      "|s_2457_113_5|Dont you think pr...|      false|                0.0|\n",
      "+------------+--------------------+-----------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_with_ratio.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5eb3ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 25:===========================================>              (3 + 1) / 4]\r"
     ]
    }
   ],
   "source": [
    "df_with_ratio.filter((col(\"self_ref_ratio\") > 0) & (~col(\"is_self_ref\"))).show(\n",
    "    10, truncate=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1669409c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 25:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------+--------------+\n",
      "|DOCNO       |TEXT                                                                                                                                                                       |is_self_ref|self_ref_ratio|\n",
      "+------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------+--------------+\n",
      "|s_2457_131_0| gt;I can imagine that the forces are far greater - a 3ft long blade (from root to tip) on an aircraft is nothing compared to the 300+ foot long monsters on wind turbines.|true       |0.0           |\n",
      "|s_3020_36_0 |[Translation](  gt;my method of healing as of late!!!                                                                                                                      |true       |0.0           |\n",
      "|s_3020_51_0 |[Translation](  gt;my selfies heh                                                                                                                                          |true       |0.0           |\n",
      "|s_3020_78_1 | gt;I'm so so glad that MOAs liked it  gt;There really isn't much time left now, for real.                                                                                 |true       |0.0           |\n",
      "|s_3020_82_0 |uuuuh...[i'm sorry](                                                                                                                                                       |true       |0.0           |\n",
      "+------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/28 02:26:34 WARN PythonRunner: Detected deadlock while completing task 2.0 in stage 25 (TID 113): Attempting to kill Python Worker\n",
      "\r\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_with_ratio.filter((col(\"is_self_ref\")) & (col(\"self_ref_ratio\") == 0)).show(\n",
    "    10, truncate=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89763fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erisk-env",
   "language": "python",
   "name": "erisk-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
