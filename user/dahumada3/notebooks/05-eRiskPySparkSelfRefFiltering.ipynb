{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5152580",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00-erisk25task1EDA.ipynb     04-eRiskAnalysisSelfReferential.ipynb\n",
      "01-erisktokenestimate.ipynb  05-eRiskPySparkSelfRefFiltering.ipynb\n",
      "03-pyterrier-test2.ipynb\n",
      "/storage/home/hcoda1/6/dahumada3/clef/erisk-2025/user/dahumada3/notebooks\n"
     ]
    }
   ],
   "source": [
    "!ls\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "916cfeae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/26 23:24:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/03/26 23:24:32 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://atl1-1-02-006-12-2.pace.gatech.edu:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[24]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>clef</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1555259b4880>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"/storage/home/hcoda1/6/dahumada3/clef/erisk-2025\")\n",
    "\n",
    "from erisk.spark import get_spark\n",
    "\n",
    "spark = get_spark()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b2a5214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set paths\n",
    "LABELS_MAJ_PATH = \"/storage/home/hcoda1/6/dahumada3/erisk_shared/raw/training_data/2023/g_qrels_majority_2.csv\"\n",
    "LABELS_CONS_PATH = \"/storage/home/hcoda1/6/dahumada3/erisk_shared/raw/training_data/2023/g_rels_consenso.csv\"\n",
    "PARQUET_DIR = \"/storage/home/hcoda1/6/dahumada3/erisk_shared/parquet/training_data/2023/partitions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c512aba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r\n",
      "\r\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DOCNO: string (nullable = true)\n",
      " |-- TEXT: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------------------------------------------------------------------------------+\n",
      "|DOCNO       |TEXT                                                                                 |\n",
      "+------------+-------------------------------------------------------------------------------------+\n",
      "|s_2457_109_0|LADWP, Inyo agree to test run on well 395                                            |\n",
      "|s_2457_110_0|State representatives recognize NIHDs District of Year designation                   |\n",
      "|s_2457_111_0|Small plane crashes en route from Bishop to Nanaimo, BC (Canada)                     |\n",
      "|s_2457_112_0|Audio and video production professionals, as an example: https://youtu.be/jv5HIrOrn2o|\n",
      "|s_2457_113_0|Sure, its a professional powerhouse for audio and video production professionals.    |\n",
      "+------------+-------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(PARQUET_DIR)\n",
    "df.printSchema()\n",
    "df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41de2608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (4,264,693 rows, 2 columns)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:=================================================>       (21 + 3) / 24]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------+----------+\n",
      "|       avg_length|min_length|max_length|\n",
      "+-----------------+----------+----------+\n",
      "|86.27327078408692|         1|     39999|\n",
      "+-----------------+----------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import length, avg, min, max\n",
    "\n",
    "row_count = df.count()\n",
    "col_count = len(df.columns)\n",
    "\n",
    "print(f\"Shape: ({row_count:,} rows, {col_count} columns)\")\n",
    "\n",
    "df_stats = df.withColumn(\"text_length\", length(\"TEXT\"))\n",
    "\n",
    "summary = df_stats.agg(\n",
    "    avg(\"text_length\").alias(\"avg_length\"),\n",
    "    min(\"text_length\").alias(\"min_length\"),\n",
    "    max(\"text_length\").alias(\"max_length\"),\n",
    ")\n",
    "\n",
    "summary.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e06f882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------+\n",
      "|TEXT                                                                             |\n",
      "+---------------------------------------------------------------------------------+\n",
      "|LADWP, Inyo agree to test run on well 395                                        |\n",
      "|State representatives recognize NIHDs District of Year designation               |\n",
      "|Small plane crashes en route from Bishop to Nanaimo, BC (Canada)                 |\n",
      "|Audio and video production professionals, as an example:                         |\n",
      "|Sure, its a professional powerhouse for audio and video production professionals.|\n",
      "+---------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:==============================================>         (20 + 4) / 24]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining rows: 4264560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "[Stage 11:=================================================>      (21 + 3) / 24]\r\n",
      "\r\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Dataframe cleanup\n",
    "from pyspark.sql.functions import col, trim\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "\n",
    "df_clean = df.filter(trim(col(\"TEXT\")) != \"\")\n",
    "df_clean = df_clean.withColumn(\"TEXT\", regexp_replace(\"TEXT\", r\"http\\S+|www\\S+\", \"\"))\n",
    "df_clean.select(\"TEXT\").show(5, truncate=False)\n",
    "\n",
    "print(\"Remaining rows:\", df_clean.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6606ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "SELF_REF_PATTERN = r\"\\b(i|me|my|mine|myself|i'm|i’ve|i'd|i’ll|i am|i was)\\b\"\n",
    "df_with_flag = df_clean.withColumn(\"is_self_ref\", col(\"TEXT\").rlike(SELF_REF_PATTERN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1a052fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:===================================================>    (22 + 2) / 24]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self-referential posts: 434,789 out of 4,264,560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "[Stage 17:=====================================================>  (23 + 1) / 24]\r\n",
      "\r\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "self_ref_df = df_with_flag.filter(col(\"is_self_ref\"))\n",
    "\n",
    "total_count = df_with_flag.count()\n",
    "self_ref_count = self_ref_df.count()\n",
    "\n",
    "print(f\"Self-referential posts: {self_ref_count:,} out of {total_count:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61d3c9d",
   "metadata": {},
   "source": [
    "PySpark filters more sentences out than pandas, since PySpark doesn't support re.IGNORECASE directly, but you can lowercase the text before applying the regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0cbd650b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lower\n",
    "\n",
    "SELF_REF_PATTERN = r\"\\b(i|me|my|mine|myself|i'm|i’ve|i'd|i’ll|i am|i was)\\b\"\n",
    "\n",
    "df_with_flag = df_clean.withColumn(\n",
    "    \"is_self_ref\", lower(col(\"TEXT\")).rlike(SELF_REF_PATTERN)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fdb6867a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 23:=====================================================>  (23 + 1) / 24]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self-referential posts: 1,191,200 out of 4,264,560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "self_ref_df = df_with_flag.filter(col(\"is_self_ref\"))\n",
    "\n",
    "total_count = df_with_flag.count()\n",
    "self_ref_count = self_ref_df.count()\n",
    "\n",
    "print(f\"Self-referential posts: {self_ref_count:,} out of {total_count:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "229131d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "SELF_REF_WORDS = set(\n",
    "    [\"i\", \"me\", \"my\", \"mine\", \"myself\", \"i'm\", \"i’ve\", \"i'd\", \"i’ll\", \"i am\", \"i was\"]\n",
    ")\n",
    "\n",
    "\n",
    "def fast_self_ref_ratio(text):\n",
    "    if not text:\n",
    "        return 0.0\n",
    "    words = text.lower().split()\n",
    "    if not words:\n",
    "        return 0.0\n",
    "    count = sum(1 for word in words if word in SELF_REF_WORDS)\n",
    "    return count / len(words)\n",
    "\n",
    "\n",
    "self_ref_ratio_udf = udf(fast_self_ref_ratio, DoubleType())\n",
    "\n",
    "df_final = df_with_flag.withColumn(\"self_ref_ratio\", self_ref_ratio_udf(col(\"TEXT\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c2dc790a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+-----------+--------------+\n",
      "|       DOCNO|                TEXT|is_self_ref|self_ref_ratio|\n",
      "+------------+--------------------+-----------+--------------+\n",
      "|s_2457_109_0|LADWP, Inyo agree...|      false|           0.0|\n",
      "|s_2457_110_0|State representat...|      false|           0.0|\n",
      "|s_2457_111_0|Small plane crash...|      false|           0.0|\n",
      "|s_2457_112_0|Audio and video p...|      false|           0.0|\n",
      "|s_2457_113_0|Sure, its a profe...|      false|           0.0|\n",
      "+------------+--------------------+-----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_final.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f1a97796",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[1;32m      3\u001b[0m nlp \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men_core_web_sm\u001b[39m\u001b[38;5;124m\"\u001b[39m, disable\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mner\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparser\u001b[39m\u001b[38;5;124m\"\u001b[39m])  \u001b[38;5;66;03m# Faster\u001b[39;00m\n\u001b[1;32m      5\u001b[0m SELF_REF_WORDS \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mme\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmy\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmine\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmyself\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi’ve\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124md\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi’ll\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi am\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi was\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      8\u001b[0m }\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"parser\"])  # Faster\n",
    "\n",
    "SELF_REF_WORDS = {\n",
    "    \"i\",\n",
    "    \"me\",\n",
    "    \"my\",\n",
    "    \"mine\",\n",
    "    \"myself\",\n",
    "    \"i'm\",\n",
    "    \"i’ve\",\n",
    "    \"i'd\",\n",
    "    \"i’ll\",\n",
    "    \"i am\",\n",
    "    \"i was\",\n",
    "}\n",
    "\n",
    "\n",
    "def spacy_self_ref_ratio(text):\n",
    "    doc = nlp(text.lower())\n",
    "    tokens = [token.text for token in doc if not token.is_space]\n",
    "    if not tokens:\n",
    "        return 0.0\n",
    "    self_ref_count = sum(1 for token in tokens if token in SELF_REF_WORDS)\n",
    "    return self_ref_count / len(tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erisk-2025",
   "language": "python",
   "name": "erisk-2025"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
